---
title: 金融领域强化学习最新进展
date: 
    created: 2024-09-27
    updated: 2024-09-27
authors: 
  - zzh
categories:
  - Reinforcement Learning
---

# Recent advances in reinforcement learning in finance

这是一篇金融领域强化学习的综述（2021）。经典的随机控制理论和其他分析方法解决金融决策问题，严重依赖模型假设。而强化学习（RL）的新发展能够用较少的模型假设，充分利用大量的金融数据，并在复杂的金融环境中改进决策。

<!-- more -->

**Autor：** Ben Hambly, Renyuan Xu, Huining Yang

## 2.强化学习基础

### 2.1 Setup:Markov decision processes

投资组合优化问题，通常有两类问题，一类是无限时间范围的马尔科夫决策过程(MDPs, Markov decision processes)，另一类则是有限时间范围的MDPs。前者主要研究长期投资策略，如养老金问题。而后者主要研究短期内的交易，如资产购买或清算的最优执行问题，如果目标金额没有被完全执行，则可能会在终止时间收到处罚。

#### *Infinite time horizon and discounted reward.*

首先考虑无**限时间范围和折扣奖励的**的**离散时间MDP**。

* 状态空间 $\mathcal{S}$，Markov process取值的空间
* 动作集合 $\mathcal{A}$，通过在集合中采取动作影响演化
* 目的是通过选择一个策略（沿着时间的行动顺序）来优化系统的预期回报。

通过定义值函数来数学的表示这一点

Define value function $V^*$ for each $s \in \mathcal{S}$ to be

\begin{equation}
    V^{*}(s)=\sup _{\Pi} V^{\Pi}(s):=\sup _{\Pi} \mathbb{E}^{\Pi}\left[\sum_{t=0}^{\infty} \gamma^{t} r\left(s_{t}, a_{t}\right) \mid s_{0}=s\right],
\end{equation}

subject to

\begin{equation}
    s_{t+1} \sim P\left(s_{t}, a_{t}\right), \quad a_{t} \sim \pi_{t}\left(s_{t}\right) .
\end{equation}

以下做一些记号上的约定：

* $\mathbb{E}^{\Pi}$ 表示在策略 ${\Pi}$ 下的期望，其中概率测度 $\mathcal{P}$ 描述MDP中的动态和奖励。
* 一个空间 $\mathcal{X}$ 的概率测度写为 $\mathcal{P}(\mathcal{X})$ 
* 状态空间 $(\mathcal{S},d_\mathcal{S})$ 和动作空间 $(\mathcal{A},d_\mathcal{A})$ 都是完全可分的度量空间，包括 $\mathcal{S}$ 和 $\mathcal{A}$ 都是有限的情况，这在RL文献中常见。
* $\gamma \in (0,1)$ 是一个折现因子
* $s_t \in \mathcal{S}$：$t$ 时刻的状态
* $a_t \in \mathcal{A}$：$t$ 时刻的动作
* $P: S \times \mathcal{A} \rightarrow \mathcal{P}(S)$ 马尔可夫过程下的转移函数
* 记号 $s_{t+1} \sim P\left(s_{t}, a_{t}\right)$ 表示 $s_{t+1}$ 从分布 $P(s_t,a_t)\in \mathcal{P}(\mathcal{S})$ 中进行采样。
* 奖励 $r(s,a)$ 是每对$(s,a)\in \mathcal{S}\times \mathcal{A}$ 的 $\mathbb{R}$ 中的一个随机变量
* 策略 $\Pi=\left\{\pi_{t}\right\}_{t=0}^{\infty}$ 是马尔可夫的，因为它只依赖于当前状态，并且可以确定性的，也可以是随机的。
:   * 对于确定性的策略，$\pi_{t}: S \rightarrow \mathcal{A}$ 将当前状态 $s_t$ 映射到一个确定性的动作
:   * 对于随机策略，$\pi_{t}: S \rightarrow \mathcal{P}(\mathcal{A})$ 将当前状态 $s_t$ 映射到一个动作空间上的分布。
:   &emsp;&emsp;&emsp; $\pi_t(s)\in\mathcal{P}(\mathcal{A})$ 表示给定状态 $s$ 在行动空间上的分布
:   &emsp;&emsp;&emsp; $\pi_t(s,a)$ 表示在状态 $s$ 采取动作 $a$ 的概率

在带有无限时间范围情况下，假设奖励 $r$ 和转移动态 $P$ 是时间齐次的，
这在马尔可夫决策过程文献中是一个标准假设。
此外，如果考虑最小化成本而不是最大化奖励，问题的设置本质上是相同的。

著名的动态规划原则(dynamic programming principle, DPP)，
即最优策略可以通过最大化一步的奖励然后从新的状态进行最优化处理，
可以用来推导值函数(1)的贝尔曼方程

\begin{equation}
    V^{*}(s)=\sup _{a \in \mathcal{A}}\left\{\mathbb{E}[r(s, a)]+\gamma \mathbb{E}_{s^{\prime} \sim P(s, a)}\left[V^{*}\left(s^{\prime}\right)\right]\right\} .
\end{equation}

可以将值函数写成

\[V^{*}(s)=\sup _{a \in \mathcal{A}} Q^{*}(s, a) ,\]

其中，$Q$-function，用于RL的基本量之一，定义如下

\begin{equation}
    Q^{*}(s, a)=\mathbb{E}[r(s, a)]+\gamma \mathbb{E}_{s^{\prime} \sim P(s, a)}\left[V^{*}\left(s^{\prime}\right)\right],
\end{equation}

在状态 $s$ 下采取动作 $a$ 的预期奖励，后再采取最优化策略。
对于 $Q$-function，也有贝尔曼方程，如下：

\begin{equation}
    Q^{*}(s, a)=\mathbb{E}[r(s, a)]+\gamma \mathbb{E}_{s^{\prime} \sim P(s, a)} \sup _{a^{\prime} \in \mathcal{A}} Q^{*}\left(s^{\prime}, a^{\prime}\right) .
\end{equation}

这使我们能够从 $Q(s,a)$ 中提取最优（平稳）策略
$\pi^*(s,a)$（如果存在的话），具体为

\[\pi^{*}(s, a) \in \arg \max _{a \in \mathcal{A}} Q(s, a) .\]

此处的无限时间范围采取的折现奖励的设置，还有另一种设置是平均奖励，也被称为遍历式奖励，这种遍历式的设置通常与金融应用不相关。

#### *Finite time horizon.*

有限时间范围 $T<\infty$ ，不再贴现未来的价值并且有一个终止奖励。有限时间范围的MDP问题可以表示如下：

\begin{equation}
    V_{t}^{*}(s)=\sup _{\Pi} V_{t}^{\Pi}(s):=\sup _{\Pi} \mathbb{E}^{\Pi}\left[\sum_{u=t}^{T-1} r_{u}\left(s_{u}, a_{u}\right)+r_{T}\left(s_{T}\right) \Bigg| s_{t}=s\right], \forall s \in \mathcal{S},
\end{equation}

subject to

\begin{equation}
    s_{u+1} \sim P_{u}\left(s_{u}, a_{u}\right), \quad a_{u} \sim \pi_{u}\left(s_{u}\right), \quad t \leq u \leq T-1.
\end{equation}

在无限时间情况的例子中，我们用 $s_u\in \mathcal{S}$ 和 $a_u\in \mathcal{A}$ 表示 agent 在时间 $u$ 的状态和动作。
然而，有限时间与无限时间有一点不同的是，我们允许时间依赖性的转移和奖励函数。

* $P_{u}: S \times \mathcal{A} \rightarrow \mathcal{P}(S)$ 表示转移函数
* $r_u(s,a)$ 对每一对$(s,a)\in \mathcal{S}\times \mathcal{A}$ 是一个实值的随机变量，$t\leq u \leq T-1$
* $r_T(s)$ 终端奖励，对所有的 $s\in\mathcal{S}$ 是一个实值随机变量
* $\Pi=\left\{\pi_{u}\right\}_{t=0}^{T}$ 马尔可夫策略可以是确定性的也可以是随机的

值函数(6)对应的贝尔曼方程定义如下：

\begin{equation}
    \begin{split}
        V_{t}^{*}(s) &=\sup _{a \in \mathcal{A}}\left\{\mathbb{E}\left[r_{t}(s, a)\right]+\mathbb{E}_{s^{\prime} \sim P_{t}(s, a)}\left[V_{t+1}^{*}\left(s^{\prime}\right)\right]\right\}, \\
        V_{T}^{*}(s) &=\mathbb{E}\left[r_{T}(s)\right]\ \ \ \ \text{(terminal condition)}
    \end{split}
\end{equation}

可以将值函数写成

\[V_{t}^{*}(s)=\sup _{a \in \mathcal{A}} Q_{t}^{*}(s, a),\]

其中 $Q_t^*$ 函数定义如下：

\begin{equation}
    Q_{t}^{*}(s, a)=\mathbb{E}\left[r_{t}(s, a)\right]+\mathbb{E}_{s^{\prime} \sim P_{t}(s, a)}\left[V_{t}^{*}\left(s^{\prime}\right)\right].
\end{equation}

$Q$-function 的贝尔曼方程由下式给出

\begin{equation}
    \begin{split}
        Q_{t}^{*}(s, a) &=\mathbb{E}\left[r_{t}(s, a)\right]+\mathbb{E}_{s^{\prime} \sim P_{t}(s, a)}\left[\sup _{a^{\prime} \in \mathcal{A}} Q_{t+1}^{*}\left(s^{\prime}, a^{\prime}\right)\right],\\
        Q_{T}^{*}(s, a) &=\mathbb{E}\left[r_{T}(s)\right]\ \ \ \ \text{for all } a \in \mathcal{A}
    \end{split}
\end{equation}

??? Note
    金融时间序列数据通常是非平稳的，因此(6)和(7)式中的时间变化转移核和价格函数对于金融应用尤为重要。

对于一个具有有限状态和动作空间以及有限奖励 $r$ 的无限时间范围的马尔可夫决策过程（MDP），
一个进一步有用的观察是，只要存在最优策略，该MDP总是具有一个平稳的最优策略。

***
**Theorem 2.1** Theorem 6.2.7 in Puterman (2014). Assume $|\mathcal{A}|<\infty$, 
$|\mathcal{S}|<\infty$, and $|r|<\infty$ with
probability one. For any infinite horizon discounted MDP, 
there always exists a deterministic stationary policy that is optimal.

***

定理 2.1 意味着总是存在一个固定的策略，可以在每个时间步长执行该策略指定的操作最大化折扣奖励。
angent 不需要随时间改变策略。平均奖励的情况也有一个类似的结果，见 Theorem 8.1.2 in Puterman (2014).

这将寻找最优顺序决策策略问题减少到寻找最优平稳策略的问题。因此，记 $\pi: S \rightarrow \mathcal{P}(\mathcal{A})$ (没有时间指数) 为一个平稳策略。

#### *Linear MDPs and linear functional approximation.*

!!! Note
    在线性马尔可夫决策过程（MDP）中，转移核（transition kernels）和奖励函数（reward function）被假设为相对于某些特征映射是线性的（Bradtke & Barto, 1996；Melo & Ribeiro, 2007）。

在**无限时间范围的背景**下，若存在 $d$ 个在 $\mathcal{S}$ 上未知(符号)测度 $\mu=\left(\mu^{(1)}, \ldots, \mu^{(d)}\right)$ ，以及一个未知的向量 $\theta\in\mathbb{R}^d$，满足对任意的 $(s,a)\in \mathcal{S}\times\mathcal{A}$，有

\begin{equation}
    P(\cdot \mid s, a)=\langle\phi(s, a), \mu(\cdot)\rangle, \quad r(s, a)=\langle\phi(s, a), \theta\rangle .
\end{equation}

则称该MDP为具有特征映射的特征映射 $\phi:\mathcal{S}  \times \mathcal{A} \rightarrow \mathbb{R}^{d}$ 的**线性MDP**。

与之相似，在**有限时间背景**下，若对于任意的 $0\leq t \leq T$，存在 $d$ 个在 $\mathcal{S}$上的 未知(符号)测度 $\mu_{t}=\left(\mu_{t}^{(1)}, \ldots, \mu_{t}^{(d)}\right)$
以及一个未知的向量 $\theta_t \in\mathbb{R}^d$，
满足对任意的 $(s,a)\in \mathcal{S}\times\mathcal{A}$，有

\begin{equation}
    P_{t}(\cdot \mid s, a)=\left\langle\phi(s, a), \mu_{t}(\cdot)\right\rangle, \quad r_{t}(s, a)=\left\langle\phi(s, a), \theta_{t}\right\rangle
\end{equation}

则称该MDP为具有特征映射的特征映射 $\phi:\mathcal{S}  \times \mathcal{A} \rightarrow \mathbb{R}^{d}$ 的**线性MDP**。

通常假设这些特征(features)是 agent 已知且有界的，也就是说 $\|\phi(s, a)\| \leq 1$ 对所有的 $(s,a)\in\mathcal{S}\times\mathcal{A}$。

线性MDP框架与具有线性泛函逼近的RL的文献密切相关，其中的值函数假设为形式

For the infinite horizon case,

\begin{equation}
    Q(s, a)=\langle\psi(s, a), \omega\rangle, \quad V(s)=\langle\xi(s), \eta\rangle
\end{equation}


For the finite horizon case,

\begin{equation}
    Q_{t}(s, a)=\left\langle\psi(s, a), \omega_{t}\right\rangle, \quad V_{t}(s)=\left\langle\xi(s), \eta_{t}\right\rangle, \forall 0 \leq t \leq T
\end{equation}

* $\psi: S \times \mathcal{A} \rightarrow \mathbb{R}^{d}$ 和 $\xi: S \rightarrow \mathbb{R}^{d}$ 是已知的特征映射
* $\omega, \omega_t,\eta$ 和 $\eta_t$ 是未知向量

在温和条件下，线性马尔可夫决策过程(即(11)或(12))与
线性函数逼近形式(即(13)或(14))是等价的（Jin et al., 2020；Yang & Wang, 2019）。


#### *Nonlinear functional approximation.*

与线性泛函近似相比，非线性泛函近似不需要先验的 kernel functions 知识。