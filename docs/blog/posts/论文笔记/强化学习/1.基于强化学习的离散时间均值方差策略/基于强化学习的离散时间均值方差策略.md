---
title: 基于强化学习的离散时间均值方差策略
date:
  created: 2024-09-16
  updated: 2024-09-16

categories:
  - 强化学习
---

# Discrete-Time Mean-Variance Strategy Based on Reinforcement Learning

**Key Words:** 
<!-- more -->

**Autor：** 


[TODO]这篇综述可以看一看
Hambly, B., Xu, R., and Yang, H. (2023). Recent advances in reinforcement learning in
finance. Mathematical Finance, 33(3):437–503.


## 2 离散时间探索性的均值方差问题
* 2.1 经典离散时间均值方差问题
* 2.2 离散时间探索性的均值方差问题

### 2.1 经典离散时间均值方差问题

首先考虑由两个资产组成的市场：

* 无风险资产，回报率 $r_f$
* 风险资产，t到t+1的超额回报率 $r_t$，服从均值为 $a$，方差为 $\sigma^2$ 的正态分布。

**假设：** $r_t,t=0,1,\dots,T-1$  是统计上独立的

* 投资者进入市场的初始财富为 $x_0$
* 投资者希望在策略 $\mathbf{u} = \{u_0, u_1,\dots,u_{T-1} \}$ 下的最终财富期望值为 $x_T = b$

简单说，投资者将会面临以下的优化问题：

\begin{align*}
        \min\ &\operatorname{Var}(x_T^u), \\[2mm]
        \text{s.t. }&\mathbb{E}[x_T^u] = b,\\[2mm]
        &x_{t+1}= r_f x_t + r_t u_t,\quad\quad t = 0,1,\dots,T-1
\end{align*}

引入 Lagrange multiplier $w$，将问题转换为无约束条件问题。

\begin{equation}
    \min_u\ \mathbb{E}(x_T^u)^2-b^2-2w(\mathbb{E}[x_T^u]-b) = \min_u\ \mathbb{E}(x_T^u - w)^2 - (w-b)^2.
\end{equation}

该问题存在解析解 $\mathbf{u}^*=\{u_0^*,u_1^*,\dots,u_{T-1}^*\}$ 依赖于 $w$.
详细的推导可见 *(2000)Optimal dynamic portfolio selection: Multiperiod mean-variance formulation*

### 2.2 离散时间探索性的均值方差问题

在强化学习框架中，控制过程 $\mathbf{u} = \{u_t,0\le t <T\}$ 是随机化的，代表探索和学习，从而形成了以测度值或分布控制的过程，其密度函数表示为 $\boldsymbol{\pi} = \{\pi_t,0\le t < T\}$.
因此，财富的动态表示为：

\begin{equation}
    x_{t+1}^\pi = r_f x_t^\pi + r_t u_t^\pi.
\end{equation}

**假定：**

* 超额收益 $r_t$ 服从一个均值为 $a$ 和方差 $\sigma^2$ 的分布。
* $u_t^\pi$ 是一个随机控制过程，概率密度为 $\pi_t$。
* $r_t$ 与 $u_t^\pi$ 是独立的。

那么在周期 $t$ 处，$r_t u_t^\pi$ 的条件一阶矩和二阶矩可以表示为：

\begin{align*}
    &\mathbb{E}_t[r_tu_t^\pi] = \mathbb{E}_t[r_t]\mathbb{E}_t[u_t^\pi] = a\int_{\mathbb{R}}u\pi_t(u)du,\\
    &\mathbb{E}_t[(r_tu_t^\pi)^2]=\mathbb{E}_t[(r_t)^2]\mathbb{E}_t[(u_t^\pi)^2]=(a^2+\sigma^2)\int_{\mathbb{R}}u^2\pi_t(u)du.
\end{align*}

随机分布控制过程 $\boldsymbol{\pi} =\{\pi_t,0 \le t < T\}$ 建模总体水平由累积熵捕捉的探索。

\begin{equation*}
    \mathcal{H}(\boldsymbol{\pi}) := -\sum_{t=0}^{T-1}\int_{\mathbb{R}}\pi_t(u)\ln\pi_t(u)du.
\end{equation*}

在离散时间市场环境下，探索性MV问题的目标函数变为：

\begin{equation}
    V^\pi = \min_\pi \mathbb{E}\left[(x_T^\pi - w)^2 + \lambda \sum_{t=0}^{T-1}\int_{\mathbb{R}}\pi_t(u)\ln\pi_t(u)du\right] - (w-b)^2.
\end{equation}

??? 原先的目标函数

    \begin{equation*}
        \min_u\ \mathbb{E}(x_T^u - w)^2 - (w-b)^2.
    \end{equation*}


* $\lambda :$ temperature parameter measures the trade-off between exploitation and exploration in this MV problem.

问题(3)中理论的最优反馈控制和相应的最优值函数如下：

定义在策略 $\boldsymbol{\pi}$ 下的值函数 $J(t,x;w)$:

\begin{equation*}
    J(t,x;w) = \mathbb{E}\left[(x_T^\pi - w)^2 + \lambda \sum_{s=t}^{T-1}\int_{\mathbb{R}}\pi_t(u)\ln\pi_t(u)du \mid x_t^\pi = x \right] - (w-b)^2
\end{equation*}

$J^*(t,x;w)$ 称为问题(3)的最优值函数：

\begin{align*}
    &J^*(t,x;w) = \min_{\pi_{t}, \ldots, \pi_{T-1}}\mathbb{E}\left[(x_T^\pi - w)^2 + \lambda \sum_{s=t}^{T-1}\int_{\mathbb{R}}\pi_t(u)\ln\pi_t(u)du \mid x_t^\pi = x \right] - (w-b)^2,\\
    &J^*(T,x;w) = (x-w)^2 - (w-b)^2.
\end{align*}


***
**Theorem 1.** At period t, the optimal value function is given by

\begin{equation}
    \begin{split}
        &J^*(t,x;w) \\
        =&\left(\frac{\sigma^2 r_f^2}{a^2+\sigma^2}\right)^{T-t}(x-\rho_t w)^2 + \frac{\lambda}{2}(T-t)\ln\left(\frac{a^2 + \sigma^2}{\pi \lambda}\right) \\
        &+ \frac{\lambda}{2}\sum_{i=t+1}^{T}(T-i)\ln\left(\frac{\sigma^2 r_f^2}{a^2 + \sigma^2}\right) - (w-b)^2.
    \end{split}
\end{equation}

where $\rho_t = (r_f^{-1})^{T-t}$. Moreover, the optimal feedback control is Gaussian, with its density function given by

\begin{equation}
    \pi^*(u;t,x,w) = \mathcal{N}\left(u \mid - \frac{ar_f(x-\rho_t w)}{a^2 + \sigma^2}, \frac{\lambda}{2(a^2 + \sigma^2)}\left( \frac{a^2 + \sigma^2}{\sigma^2 r_f^2} \right)^{T-t-1}\right).
\end{equation}
***

[NOTE:超额收益 $r_t$ 服从一个均值为 $a$ 和方差 $\sigma^2$ 的分布。]


??? 定理1有两点需要注意
    1. 方差项衡量最优的高斯策略在 $t$ 时刻的的探索程度，意味着探索会随着时间衰减。agent 初始会最大程度的进行探索，然后探索程度会随时间逐渐衰减。
    随着时间接近到期日，利用会主导探索并且会变得越来越重要，这是因为有一个将对投资者的行动进行评估的 deadline T。
    2. 最优高斯策略的均值与探索权重 $\lambda$ 无关，方差与状态 $x$ 无关。完美的将**利用**和**探索**进行了分离，因为前者由均值捕捉，后者由方差捕捉。



## 3. 离散时间算法

